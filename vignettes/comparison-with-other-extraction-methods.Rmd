---
title: "Comparison with other extraction methods"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Comparison with other extraction methods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warnings = FALSE,
  out.width=900,
  out.height=600,
  fig.width=9,
  fig.height=6
)
```


There are many existing R packages to extract raster data at locations of vector
geometries (points, lines or polygons). However, when your vector geometries
are combined with time (single datetimes or intervals), large raster files and 
many combinations of space and time to be extracted, the default strategies to 
use these packages are slow or use too much RAM (often causing crashes).

Envfetch uses `sf` , `terra` and `stars` package in an optimal way
for fast and memory-efficient extraction of raster data over space and time.
Each optimisation has been implemented in the simple `envfetch()` function, and
this vignette functions as a reasoning for these optimisations.
For a guide on how to use `envfetch()` optimisations automatically applied, see 
[]().


All below code examples use a mock dataset created with the following code:
```{r, cache=TRUE}
library(envfetch)
library(ggplot2)
library(lubridate)
library(terra)
library(stars)

terraOptions(progress=0)

num_benchmark_repeats <- 5

tif = system.file("tif/L7_ETMs.tif", package = "stars")

dates <- seq(as.Date('2018-01-01'), as.Date('2018-09-21'), 'day')

r <- rast(tif)
r <- rep(r, 44)[[1:264]]
time(r) <- dates

p <- throw(
  offset=c(288700, 9110700),
  cellsize=10000/32,
  n=32,
  time_interval=interval(start='2018-01-01', end='2018-01-01'),
  crs=st_crs(r)
)

num_intervals <- 128
interval_length <- nrow(p) / num_intervals

for (i in 1:num_intervals) {
  start_idx <- ((i-1) * interval_length) + 1
  end_idx <- i * interval_length
  start_time <- as.Date("2018-01-01") + (i-1)
  end_time <- start_time + 1
  
  p$time_column[start_idx:end_idx] <- interval(start=start_time, end=end_time)
}

p$extracted <- NA
```

```{r, cache=TRUE}
plot(r[[1]])
points(st_coordinates(p), pch = 19, col = "red")
```

## Extract once

The simplest approach to extract over space and time is to summarise rasters 
temporally and then extract spatially within the locations of your vector data.
For one time range, existing solutions are well suited to this task. Below is
an example with the `terra` package:

```{r, cache=TRUE}
one_time_range <- p
one_time_range$time_column <- interval(start='2018-01-01', end='2018-01-02')
extract_one_time_range <- function() {
  mean_r <- mean(r[[time(r) >= '2018-01-01' & time(r) <= '2018-01-02']])
  return(extract(mean_r, one_time_range))
}

head(extract_one_time_range())

microbenchmark::microbenchmark(
  extract_one_time_range(),
  times=num_benchmark_repeats
)
```


Taking our first approach to multiple time ranges starts to give us problems.
For this approach to scale, we need to repeat our extraction multiple times for
each time range. This results in us requiring approximately twice the time for 
two time ranges, triple the time for three time ranges and so on.

```{r, cache=TRUE}
extract_n_time_ranges <- function(n, p, r) {
  unique_time_ranges <- unique(p$time_column)
  unique_time_ranges <- unique_time_ranges[1:n]
  # loop through each unique time range and extract
  for(i in seq_len(length(unique_time_ranges))) {
    t <- unique_time_ranges[i]
    indx <- as.character(p$time_column) == as.character(t)
    mean_r <- mean(r[[time(r) >= int_start(t) & time(r) <= int_end(t)]])
    subset <- p[indx,]
    p$extracted[indx] <- extract(mean_r, subset, ID=FALSE)$mean
  }
  return(p)
}

bm <- microbenchmark::microbenchmark(
  extract_n_time_ranges(n = 1, p, r),
  extract_n_time_ranges(n = 2, p, r),
  extract_n_time_ranges(n = 4, p, r),
  extract_n_time_ranges(n = 8, p, r),
  extract_n_time_ranges(n = 16, p, r),
  extract_n_time_ranges(n = 32, p, r),
  extract_n_time_ranges(n = 64, p, r),
  extract_n_time_ranges(n = 128, p, r),
  times=num_benchmark_repeats
)
bm
autoplot(bm)
```


When your vector data has many time ranges, a faster solution is to instead do
one single extraction for all time ranges and then summarise the data. This
avoids repeating the expensive extraction operation.

```{r, cache=TRUE}
extract_once <- function(n, p, r) {
  # subset to only have the time columns we are interested in
  time_column <- p$time_column
  unique_time_ranges <- unique(time_column)
  index <- time_column %in% unique_time_ranges[1:n]
  p <- p[index,]
  
  x <- sf::st_drop_geometry(p)
  
  extracted <- extract(r, p, ID=FALSE)
  times <- time(r)

  return(
    envfetch:::non_vectorised_summarisation(
      x = x,
      extracted = extracted,
      IDs = 1:nrow(x),
      temporal_fun = mean,
      tms = times,
      nms = names(extracted),
      time_column_name = 'time_column',
      new_col_names = c('L7_ETMs'),
      multi_values_in_extraction_per_row = FALSE
    )
  )
}

extracted_multiple_times <- extract_n_time_ranges(n = 128, p, r)
extracted_once <- extract_once(n = 128, p, r)

identical(extracted_multiple_times$extracted, extracted_once$L7_ETMs)

bm <- microbenchmark::microbenchmark(
  extract_n_time_ranges(n = 128, p, r),
  extract_once(n = 128, p, r),
  times=num_benchmark_repeats
)
bm
autoplot(bm)
```


## Only extract the time slices you are interested in

Although it's faster to extract once, it may still be slow in cases where the
raster stack is very large or times to extract are far from one another in the
raster stack.

The solution is that we do not need to extract all time slices of the data.
A simple optimisation is to trim the time range of the raster around the
minimum and maximum of our vector data, e.g. 
`r[[times(r) > min_time & times(r) < min_time]]`. This would help if the time 
ranges you wish to extract are close together within a raster stack.

But to also optimise in other cases, especially when time ranges are far from
one another, we can only extract from the time slices we are interested 
in. This is achievable by sub-setting the raster before the extraction with
only the relevant time slices (calculated by the `find_relevant_time_slices()` 
function in `envfetch`).

Note, time to calculate the relevant time slices can be more than you
benefit from the reduced extraction if time ranges to extract are close to one 
another and the raster stack is small. We can see this in the below plot where
extracting only what we need only has a benefit when extracting from a
large raster with time slices far from one another.

```{r, cache=TRUE}
large_p <- p
large_p$time_column <- interval(
  int_start(large_p$time_column) + years(5),
  int_end(large_p$time_column) + years(5)
)
  
large_p <- rbind(p, large_p)
dates <- seq(min(int_start(large_p$time_column)), max(int_end(large_p$time_column)), 'day')
large_r <- rast(tif)
large_r <- rep(large_r, 326)[[1:1955]]
time(large_r) <- dates

extract_only_what_we_need <- function(n, p, r) {
  # subset to only have the time columns we are interested in
  time_column <- p$time_column
  unique_time_ranges <- unique(time_column)
  index <- time_column %in% unique_time_ranges[1:n]
  p <- p[index,]
  
  x <- sf::st_drop_geometry(p)

  relevant_time_slices <- envfetch:::find_relevant_time_slices(time(r), x$time_column)
  
  r <- r[[relevant_time_slices]]
  
  extracted <- extract(r, p, ID=FALSE)
  times <- time(r)
  
  return(
    envfetch:::non_vectorised_summarisation(
      x = x,
      extracted = extracted,
      IDs = 1:nrow(x),
      temporal_fun = mean,
      tms = times,
      nms = names(extracted),
      time_column_name = 'time_column',
      new_col_names = c('L7_ETMs'),
      multi_values_in_extraction_per_row = FALSE
    )
  )
}

extracted_only_what_we_need <- extract_only_what_we_need(n = 128, p, r)

identical(extracted_multiple_times$extracted, extracted_only_what_we_need$L7_ETMs)

bm <- microbenchmark::microbenchmark(
  extract_n_time_ranges(n = 128, p, r),
  extract_once(n = 128, p, r),
  extract_only_what_we_need(n = 128, p, r),
  extract_n_time_ranges(n = 256, large_p, large_r),
  extract_once(n = 256, large_p, large_r),
  extract_only_what_we_need(n = 256, large_p, large_r),
  times=num_benchmark_repeats
)
bm
autoplot(bm)
```

For cases where you intend to extract single time slices not time ranges for 
each geometry, we have also implemented a wrapper around the approach used by
`stars` in their `st_extract` function. This uses a similar approach by only 
extracting the time slices needed.


## Vectorise summarisation functions

Another optimisation is in the summarisation step. R is slow at running tasks
repeatedly (e.g. row by row). Offsetting that task to a faster programming 
language under the hood is possible with vectorised functions. Functions like
`rowMeans()` and `rowSums()` are written in C instead of R. As a result, 
they don't require as much overhead (of the R interpreter), giving us further 
performance gains.

```{r, cache=TRUE}
extract_only_what_we_need_vectorised <- function(n) {
  # subset to only have the time columns we are interested in
  time_column <- p$time_column
  unique_time_ranges <- unique(time_column)
  index <- time_column %in% unique_time_ranges[1:n]
  p <- p[index,]
  
  x <- sf::st_drop_geometry(p)

  relevant_time_slices <- envfetch:::find_relevant_time_slices(time(r), x$time_column)
  
  r <- r[[relevant_time_slices]]
  
  extracted <- extract(r, p, ID=FALSE)
  times <- time(r)
  
  return(
    envfetch:::vectorised_summarisation(
      x = x,
      extracted = extracted,
      temporal_fun = rowMeans,
      tms = times,
      nms = names(extracted),
      time_column_name = 'time_column',
      new_col_names = c('L7_ETMs')
    )
  )
}

extracted_only_what_we_need_vectorised <- extract_only_what_we_need_vectorised(n = 128)

identical(extracted_only_what_we_need$L7_ETMs, extracted_only_what_we_need_vectorised$L7_ETMs)

bm <- microbenchmark::microbenchmark(
  extract_only_what_we_need(n = 128, p, r),
  extract_only_what_we_need_vectorised(n = 128),
  times=num_benchmark_repeats
)
bm
autoplot(bm)
```


## Don't repeat yourself

Finally, using existing packages with repeated data results in repeated
extractions. That is, if you provide extraction functions with the same data 
point multiple times, it will extract that point multiple times.

```{r, cache=TRUE}
duplicated_p <- p[rep(seq_len(nrow(p)), each = 20),]

extract_duplicated_only_what_we_need_vectorised <- function(n) {
  # subset to only have the time columns we are interested in
  time_column <- duplicated_p$time_column
  unique_time_ranges <- unique(time_column)
  index <- time_column %in% unique_time_ranges[1:n]
  duplicated_p <- duplicated_p[index,]
  
  x <- sf::st_drop_geometry(duplicated_p)

  relevant_time_slices <- envfetch:::find_relevant_time_slices(time(r), x$time_column)
  
  r <- r[[relevant_time_slices]]
  
  extracted <- extract(r, duplicated_p, ID=FALSE)
  times <- time(r)
  
  return(
    envfetch:::vectorised_summarisation(
      x = x,
      extracted = extracted,
      temporal_fun = rowMeans,
      tms = times,
      nms = names(extracted),
      time_column_name = 'time_column',
      new_col_names = c('L7_ETMs')
    )
  )
}

repeated_extraction_without_duplicates <- extract_only_what_we_need_vectorised(n = 128)
repeated_extraction_with_duplicates <- extract_duplicated_only_what_we_need_vectorised(n = 128)

non_repeated_extraction_without_duplicates <- envfetch(p, r, use_cache=FALSE, verbose=FALSE)
non_repeated_extraction_with_duplicates <- envfetch(duplicated_p, r, use_cache=FALSE, verbose=FALSE)

identical(repeated_extraction_with_duplicates$L7_ETMs, non_repeated_extraction_with_duplicates$L7_ETMs)

bm <- microbenchmark::microbenchmark(
  extract_only_what_we_need_vectorised(n = 128),
  extract_duplicated_only_what_we_need_vectorised(n = 128),
  envfetch(p, r, use_cache=FALSE, verbose=FALSE),
  envfetch(duplicated_p, r, use_cache=FALSE, verbose=FALSE),
  times=num_benchmark_repeats
)
bm
autoplot(bm)
```


Similarly, if you have already extracted the data once, why extract it again? 
Saving data after an extraction and reloading it when needed is convenient and
saves time, especially if an extraction pipeline fails or crashes part of the 
way through or if you want to use the data between R sessions. For convenience,
storing data for future requests (caching) is done automatically by 
`envfetch()`.


```{r, cache=TRUE}
extracted <- envfetch(p, r)

# after running the first time above, the result is loaded straight from the cache
# and not computed again
extracted <- envfetch(p, r)
```


## > 15x speed improvement

The above optimisations result in the speed benefits found in the `envfetch`
function.

```{r, cache=TRUE, message=FALSE}
bm <- microbenchmark::microbenchmark(
  extract_n_time_ranges(n = 128, p, r),
  envfetch(p, r, use_cache = FALSE, verbose = FALSE),
  times=num_benchmark_repeats
)
bm
autoplot(bm)
```

Going from the most naive extraction solution with our small example (128 time 
intervals), we have changed our extraction time from 15 seconds to 1 second.
That is a speed up of 15x. Note, these benefits get larger both with
the number of time intervals that need to be extracted and how far in time
time intervals are from one another in the raster file. Extraction tasks that
would otherwise take multiple days can be achievable in minutes or hours with 
`envfetch`.

To ensure this extraction process is possible on lower-end machines, there are 
further strategies to optimise RAM usage described below.


## Don't overuse RAM

In an ideal world, it would be best to extract all the data necessary for the
extraction at once. However, the amount of data a computer can extract at once
is limited by how much RAM it has. 

To allow computers with less RAM to complete the same extraction task, we can
separate extractions into smaller chunks. These can then be processed one at a 
time, limiting RAM usage within the available limits. We have implemented an
automatic system to extract data in chunks in the `extract_over_space` function,
used internally by `envfetch()`.

Remember, all these optimisations are built into the `envfetch()`
function so you don't need to repeat this when extracting your dataset.


## Run on google cloud

In cases where downloading of local files for extracting data are not possible 
or practical, cloud solutions can provide a performant alternative. The largest
and most commonly used cloud-based geospatial analysis platform is Google
Earth Engine.

Currently, there is one R package with an implemented extraction function
for Google earth engine, `rgee`. However, with large datasets or extraction tasks,
standard use of the `rgee::ee_extract` function can causes crashes or overuse 
of Google Earth Engine's memory limits, failing the extraction task. 

We have implemented processing data in chunks with the `extract_gee` function,
used internally by `envfetch()`, to allow processing of more data, while 
remaining within your quota limits (https://developers.google.com/earth-engine/guides/usage#adjustable_quota_limits).

Using `ee_extract` from `rgee` will by default fail with this dataset.
```{r eval=FALSE}
rgee::ee_Initialize()
# first half of extraction process (extracting between all time ranges)
p_feature <- rgee::sf_as_ee(sf::st_geometry(large_p))

ic <- rgee::ee$ImageCollection('MODIS/061/MOD13Q1')$
  filterBounds(p_feature)$
  filterDate(
    as.character(as_date(min(int_start(p$time_column)))),
    as.character(as_date(max(int_end(p$time_column))))
  )

rgee_extracted <- rgee::ee_extract(
  x = ic,
  y = p_feature,
  scale = 250,
  fun = rgee::ee$Reducer$mean(),
  lazy = FALSE,
  sf = TRUE,
)
#> Error in py_call_impl(callable, call_args$unnamed, call_args$named) :
#> ee.ee_exception.EEException: User memory limit exceeded.
#> Run `reticulate::py_last_error()` for details.

# ... you would then also need to make a custom solution to summarise this data
```

`envfetch` will process this same task in chunks to ensure it does not fail.
It will also summarise the data for you.
```{r, cache=TRUE}
envfetch_extracted <- envfetch(large_p, 'MODIS/061/MOD13Q1')
```

The chunk size can be optimised for your particular task by modifying the
`max_chunk_time_day_range` and `max_feature_collection_size` arguments in
`envfetch()`.



